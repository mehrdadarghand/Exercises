{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a370318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c17708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df=df[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e223ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"userId\"] = df[\"userId\"].apply(lambda x: f\"user_{x}\")\n",
    "\n",
    "df[\"itemId\"] = df[\"itemId\"].apply(lambda x: f\"item_{x}\")\n",
    "\n",
    "df[\"rating\"] = df[\"rating\"].apply(lambda x: float(x))\n",
    "    \n",
    "\n",
    "ratings_group = df.sort_values(by=[\"date\"]).groupby(\"userId\")\n",
    "\n",
    "ratings_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"userId\": list(ratings_group.groups.keys()),\n",
    "        \"itemId\": list(ratings_group.itemId.apply(list)),\n",
    "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
    "        \"date\": list(ratings_group.date.apply(list)),\n",
    "    }\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 6\n",
    "step_size = 2\n",
    "     \n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) == window_size:\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "\n",
    "ratings_data.itemId = ratings_data.itemId.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "ratings_data.ratings = ratings_data.ratings.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "del ratings_data[\"date\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratings_data_item = ratings_data[[\"userId\", \"itemId\"]].explode(\n",
    "    \"itemId\", ignore_index=True\n",
    ")\n",
    "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
    "ratings_data_transformed = pd.concat([ratings_data_item, ratings_data_rating], axis=1)\n",
    "\n",
    "ratings_data_transformed.itemId = ratings_data_transformed.itemId.str.join(',')\n",
    "\n",
    "ratings_data_transformed = ratings_data_transformed.dropna(subset=['ratings'])\n",
    "\n",
    "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
    "    lambda x: \",\".join([str(v) for v in x])\n",
    ")\n",
    "\n",
    "ratings_data_transformed.rename(\n",
    "    columns={\"itemId\": \"sequence_item_ids\", \"ratings\": \"sequence_ratings\"},\n",
    "    inplace=True,\n",
    ")\n",
    "     \n",
    "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.80\n",
    "train_data = ratings_data_transformed[random_selection]\n",
    "test_data = ratings_data_transformed[~random_selection]\n",
    "\n",
    "train_data.to_csv(\"train_new.csv\", index=False, sep=\"|\", header=False)\n",
    "test_data.to_csv(\"test_new.csv\", index=False, sep=\"|\", header=False)\n",
    "\n",
    "CSV_HEADER = list(ratings_data_transformed.columns)\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"userId\": list(df.userId.unique()),\n",
    "    \"itemId\": list(df.itemId.unique()),\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    def process(features):\n",
    "        items_ids_string = features[\"sequence_item_ids\"]\n",
    "        sequence_items_ids = tf.strings.split(items_ids_string, \",\").to_tensor()\n",
    "\n",
    "        # The last item id in the sequence is the target item.\n",
    "        features[\"target_item_id\"] = sequence_items_ids[:, -1]\n",
    "        features[\"sequence_item_ids\"] = sequence_items_ids[:, :-1]\n",
    "        \n",
    "        ratings_string = features[\"sequence_ratings\"]\n",
    "        sequence_ratings = tf.strings.to_number(\n",
    "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
    "        ).to_tensor()\n",
    "\n",
    "        # The last rating in the sequence is the target for the model to predict.\n",
    "        target = sequence_ratings[:, -1]\n",
    "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
    "\n",
    "        return features, target\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim=\"|\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(process)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef34b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_input_features(inputs,include_user_id=True,include_user_features=False,include_item_features=False):\n",
    "\n",
    "    encoded_transformer_features = []\n",
    "    encoded_other_features = []\n",
    "\n",
    "    other_feature_names = []\n",
    "    if include_user_id:\n",
    "        other_feature_names.append(\"user_id\")\n",
    "    if include_user_features:\n",
    "        other_feature_names.extend(USER_FEATURES)\n",
    "\n",
    "\n",
    "    for feature_name in other_feature_names:\n",
    "        \n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
    "            inputs[feature_name]\n",
    "        )\n",
    "        \n",
    "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "        \n",
    "        embedding_encoder = layers.Embedding(\n",
    "            input_dim=len(vocabulary),\n",
    "            output_dim=embedding_dims,\n",
    "            name=f\"{feature_name}_embedding\",\n",
    "        )\n",
    "        \n",
    "        encoded_other_features.append(embedding_encoder(idx))\n",
    "    if len(encoded_other_features) > 1:\n",
    "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
    "    elif len(encoded_other_features) == 1:\n",
    "        encoded_other_features = encoded_other_features[0]\n",
    "    else:\n",
    "        encoded_other_features = None\n",
    "\n",
    "    ## Create a item embedding encoder\n",
    "    item_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"itemId\"]\n",
    "    item_embedding_dims = int(math.sqrt(len(item_vocabulary)))\n",
    "    # Create a lookup to convert string values to integer indices.\n",
    "    item_index_lookup = StringLookup(\n",
    "        vocabulary=item_vocabulary,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        name=\"item_index_lookup\",\n",
    "    )\n",
    "    \n",
    "    item_embedding_encoder = layers.Embedding(\n",
    "        input_dim=len(item_vocabulary),\n",
    "        output_dim=item_embedding_dims,\n",
    "        name=f\"item_embedding\",\n",
    "    )\n",
    "    \n",
    "    item_embedding_processor = layers.Dense(\n",
    "        units=item_embedding_dims,\n",
    "        activation=\"relu\",\n",
    "        name=\"process_item_embedding_with_genres\",\n",
    "    )\n",
    "\n",
    "    def encode_item(item_id):\n",
    "        # Convert the string input values into integer indices.\n",
    "        item_idx = item_index_lookup(item_id)\n",
    "        item_embedding = item_embedding_encoder(item_idx)\n",
    "        encoded_item = item_embedding\n",
    "        if include_item_features:\n",
    "            item_genres_vector = item_genres_lookup(item_idx)\n",
    "            encoded_item = item_embedding_processor(\n",
    "                layers.concatenate([item_embedding, item_genres_vector])\n",
    "            )\n",
    "        return encoded_item \n",
    "\n",
    "    target_item_id = inputs[\"target_item_id\"]\n",
    "    encoded_target_item = encode_item(target_item_id)\n",
    "\n",
    "    ## Encoding sequence item_ids.\n",
    "    sequence_items_ids = inputs[\"sequence_item_ids\"]\n",
    "    encoded_sequence_items = encode_item(sequence_items_ids)\n",
    "    # Create positional embedding.\n",
    "    position_embedding_encoder = layers.Embedding(\n",
    "        input_dim=sequence_length,\n",
    "        output_dim=item_embedding_dims,\n",
    "        name=\"position_embedding\",\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
    "    encodded_positions = position_embedding_encoder(positions)\n",
    "    \n",
    "    sequence_ratings = tf.expand_dims(inputs[\"sequence_ratings\"], -1)\n",
    "    \n",
    "    encoded_sequence_items_with_poistion_and_rating = layers.Multiply()(\n",
    "        [(encoded_sequence_items + encodded_positions), sequence_ratings]\n",
    "    )\n",
    "\n",
    "    # Construct the transformer inputs.\n",
    "    for encoded_item in tf.unstack(\n",
    "        encoded_sequence_items_with_poistion_and_rating, axis=1\n",
    "    ):\n",
    "        encoded_transformer_features.append(tf.expand_dims(encoded_item, 1))\n",
    "    encoded_transformer_features.append(encoded_target_item)\n",
    "\n",
    "    encoded_transformer_features = layers.concatenate(\n",
    "        encoded_transformer_features, axis=1\n",
    "    )\n",
    "\n",
    "    return encoded_transformer_features, encoded_other_features\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def create_model_inputs():\n",
    "    return {\n",
    "        \"userId\": layers.Input(name=\"userId\", shape=(1,), dtype=tf.string),\n",
    "        \"sequence_item_ids\": layers.Input(\n",
    "            name=\"sequence_item_ids\", shape=(sequence_length - 1,), dtype=tf.string\n",
    "        ),\n",
    "        \"target_item_id\": layers.Input(\n",
    "            name=\"target_item_id\", shape=(1,), dtype=tf.string\n",
    "        ),\n",
    "        \"sequence_ratings\": layers.Input(\n",
    "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
    "        )}\n",
    "     \n",
    "\n",
    "    \n",
    "include_user_id = False\n",
    "include_user_features = False\n",
    "include_item_features = False\n",
    "\n",
    "hidden_units = [256, 128]\n",
    "dropout_rate = 0.1\n",
    "num_heads = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737827d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = create_model_inputs()\n",
    "    transformer_features, other_features = encode_input_features(\n",
    "        inputs, include_user_id, include_user_features, include_item_features\n",
    "    )\n",
    "\n",
    "    # Create a multi-headed attention layer.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
    "    )(transformer_features, transformer_features)\n",
    "\n",
    "    # Transformer block.\n",
    "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "    x1 = layers.Add()([transformer_features, attention_output])\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    x2 = layers.LeakyReLU()(x1)\n",
    "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
    "    x2 = layers.Dropout(dropout_rate)(x2)\n",
    "    transformer_features = layers.Add()([x1, x2])\n",
    "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
    "    features = layers.Flatten()(transformer_features)\n",
    "\n",
    "    # Included the other features.\n",
    "    if other_features is not None:\n",
    "        features = layers.concatenate(\n",
    "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "        )\n",
    "\n",
    "    # Fully-connected layers.\n",
    "    for num_units in hidden_units:\n",
    "        features = layers.Dense(num_units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.LeakyReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec38bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d86034",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "# Read the training data.\n",
    "train_dataset = get_dataset_from_csv(\"train_new.csv\", shuffle=True, batch_size=265)\n",
    "\n",
    "# Fit the model with the training data.\n",
    "model.fit(train_dataset, epochs=5)\n",
    "\n",
    "# Read the test data.\n",
    "test_dataset = get_dataset_from_csv(\"test_new.csv\", batch_size=265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ba219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
